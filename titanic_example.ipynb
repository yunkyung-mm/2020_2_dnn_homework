{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "IE04_2019310708.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QlihzMh2rMID"
      },
      "source": [
        "###Exercise 8-1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c51B9JN9jr0B"
      },
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch import nn, from_numpy, tensor, optim, cuda\n",
        "from torchvision import datasets, transforms \n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import time\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P4fZ-BrrjtHw"
      },
      "source": [
        "class TitanicDataset(Dataset):\n",
        "    \"\"\" Titanic dataset.\"\"\"\n",
        "\n",
        "    # Initialize your data, download, etc.\n",
        "    def __init__(self):\n",
        "        df = pd.read_csv('./drive/My Drive/titanic_data/train.csv')\n",
        "        cols = ['Name', 'Ticket', 'Cabin']\n",
        "        df = df.drop(cols, axis=1)\n",
        "        df = df.dropna()\n",
        "        dummies = []\n",
        "        cols = ['Pclass', 'Sex', 'Embarked']\n",
        "        for col in cols:\n",
        "            dummies.append(pd.get_dummies(df[col]))\n",
        "        titanic_dummies = pd.concat(dummies, axis=1)\n",
        "        df = pd.concat((df,titanic_dummies), axis=1)\n",
        "        df = df.drop(['Pclass', 'Sex', 'Embarked'], axis=1)\n",
        "        df['Age'] = df['Age'].interpolate()\n",
        "        x_data = df.values\n",
        "        self.x_data = np.delete(x_data, 1, axis=1)\n",
        "        self.y_data = df['Survived'].values\n",
        "        self.len = x_data.shape[0]\n",
        "        \n",
        "    def __getitem__(self, index):\n",
        "        return self.x_data[index], self.y_data[index]\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.len"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RAuGoquJoUQU"
      },
      "source": [
        "class Model(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        \"\"\"\n",
        "        In the constructor we instantiate two nn.Linear module\n",
        "        \"\"\"\n",
        "        super(Model, self).__init__()\n",
        "        self.l0 = nn.Linear(32,13)\n",
        "        self.l1 = nn.Linear(13, 6)\n",
        "        self.l2 = nn.Linear(6, 4)\n",
        "        self.l3 = nn.Linear(4, 1)\n",
        "\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        In the forward function we accept a Variable of input data and we must return\n",
        "        a Variable of output data. We can use Modules defined in the constructor as\n",
        "        well as arbitrary operators on Variables.\n",
        "        \"\"\"\n",
        "        out1 = self.sigmoid(self.l1(x))\n",
        "        out2 = self.sigmoid(self.l2(out1))\n",
        "        y_pred = self.sigmoid(self.l3(out2))\n",
        "        return y_pred\n",
        "\n",
        "\n",
        "# our model\n",
        "model = Model()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0wLYr73HjdIX"
      },
      "source": [
        "dataset = TitanicDataset()\n",
        "train_loader = DataLoader(dataset=dataset,\n",
        "                          batch_size=32,\n",
        "                          shuffle=True,\n",
        "                          num_workers=2)\n",
        "\n",
        "\n",
        "criterion = nn.BCELoss(reduction='sum')\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(2):\n",
        "    for i, data in enumerate(train_loader, 0):\n",
        "        # get the inputs\n",
        "        inputs, labels = data\n",
        "\n",
        "        # inputs=np.float32(inputs)\n",
        "        # Forward pass: Compute predicted y by passing x to the model\n",
        "        y_pred = model(inputs.float())\n",
        "\n",
        "        # Compute and print loss\n",
        "        loss = criterion(y_pred, labels.float())\n",
        "        print(f'Epoch {epoch + 1} | Batch: {i+1} | Loss: {loss.item():.4f}')\n",
        "\n",
        "        # Zero gradients, perform a backward pass, and update the weights.\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2lG1BHnqrSUQ"
      },
      "source": [
        "###Exercise 9-1,2\n",
        "CrossEntropyLoss VS Negative Log-LikelihoodLoss\n",
        "\n",
        "\n",
        "*Any loss consisting of a negative log-likelihood is a cross entropy between the empirical distribution defined by the training set and the probability distribution defined by model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RvANAqdwkNSd"
      },
      "source": [
        "class OttoDataset(Dataset):\n",
        "    # Initialize your data, download, etc.\n",
        "    def __init__(self):\n",
        "        xy = np.loadtxt('./drive/My Drive/otto_data/train.csv',\n",
        "                        delimiter=',', dtype=np.float32, skiprows=1)\n",
        "        self.len = xy.shape[0]\n",
        "        self.x_data = from_numpy(xy[:, 1:-1])\n",
        "        self.y_data = from_numpy(xy[:, [-1]])\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        return self.x_data[index], self.y_data[index]\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.len"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bMh2sFi0LdLy"
      },
      "source": [
        "batch_size = 32\n",
        "device = 'cuda' if cuda.is_available() else 'cpu'\n",
        "print(f'Training otto Model on {device}\\n{\"=\" * 44}')\n",
        "\n",
        "class Net(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.l2 = nn.Linear(416, 320)\n",
        "        self.l3 = nn.Linear(320, 240)\n",
        "        self.l4 = nn.Linear(240, 120)\n",
        "        self.l5 = nn.Linear(120, 5)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(-1, 416)  # Flatten the data (n, 1, 28, 28)-> (n, 784)\n",
        "        x = F.relu(self.l2(x.float()))\n",
        "        x = F.relu(self.l3(x))\n",
        "        x = F.relu(self.l4(x))\n",
        "        return self.l5(x)\n",
        "\n",
        "\n",
        "model = Net()\n",
        "model.to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.5)\n",
        "\n",
        "\n",
        "def train(epoch):\n",
        "    model.train()\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        print(len(output))\n",
        "        print(len(target))\n",
        "        loss = criterion(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        if batch_idx % 10 == 0:\n",
        "            print('Train Epoch: {} | Batch Status: {}/{} ({:.0f}%) | Loss: {:.6f}'.format(\n",
        "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
        "                100. * batch_idx / len(train_loader), loss.item()))\n",
        "\n",
        "\n",
        "def test():\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    for data, target in test_loader:\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        output = model(data)\n",
        "        # sum up batch loss\n",
        "        test_loss += criterion(output, target).item()\n",
        "        # get the index of the max\n",
        "        pred = output.data.max(1, keepdim=True)[1]\n",
        "        correct += pred.eq(target.data.view_as(pred)).cpu().sum()\n",
        "\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "    print(f'===========================\\nTest set: Average loss: {test_loss:.4f}, Accuracy: {correct}/{len(test_loader.dataset)} '\n",
        "          f'({100. * correct / len(test_loader.dataset):.0f}%)')\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    since = time.time()\n",
        "    for epoch in range(1, 10):\n",
        "        epoch_start = time.time()\n",
        "        train(epoch)\n",
        "        m, s = divmod(time.time() - epoch_start, 60)\n",
        "        print(f'Training time: {m:.0f}m {s:.0f}s')\n",
        "        test()\n",
        "        m, s = divmod(time.time() - epoch_start, 60)\n",
        "        print(f'Testing time: {m:.0f}m {s:.0f}s')\n",
        "\n",
        "    m, s = divmod(time.time() - since, 60)\n",
        "    print(f'Total Time: {m:.0f}m {s:.0f}s\\nModel was trained on {device}!')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x-24Tm0jNGVb"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}